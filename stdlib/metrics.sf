// SynthFlow ML Metrics Library
// Model evaluation metrics for classification, regression, and clustering

// ===== Classification Metrics =====

// Accuracy: (TP + TN) / total
fn accuracy(y_true: array, y_pred: array) -> float {
    let correct = 0
    for (let i = 0; i < len(y_true); i = i + 1) {
        if (y_true[i] == y_pred[i]) {
            correct = correct + 1
        }
    }
    return float(correct) / len(y_true)
}

// Confusion matrix for binary classification
fn confusionMatrix(y_true: array, y_pred: array) -> map {
    let tp = 0
    let tn = 0
    let fp = 0
    let fn = 0
    
    // Assume positive class is the second unique value
    let classes = unique(y_true)
    let pos_class = classes[1]
    let neg_class = classes[0]
    
    for (let i = 0; i < len(y_true); i = i + 1) {
        if (y_true[i] == pos_class && y_pred[i] == pos_class) {
            tp = tp + 1
        } else if (y_true[i] == neg_class && y_pred[i] == neg_class) {
            tn = tn + 1
        } else if (y_true[i] == neg_class && y_pred[i] == pos_class) {
            fp = fp + 1
        } else {
            fn = fn + 1
        }
    }
    
    return {
        "tp": tp,
        "tn": tn,
        "fp": fp,
        "fn": fn
    }
}

// Precision: TP / (TP + FP)
fn precision(y_true: array, y_pred: array) -> float {
    let cm = confusionMatrix(y_true, y_pred)
    if (cm.tp + cm.fp == 0) { return 0.0 }
    return float(cm.tp) / (cm.tp + cm.fp)
}

// Recall (Sensitivity): TP / (TP + FN)
fn recall(y_true: array, y_pred: array) -> float {
    let cm = confusionMatrix(y_true, y_pred)
    if (cm.tp + cm.fn == 0) { return 0.0 }
    return float(cm.tp) / (cm.tp + cm.fn)
}

// F1 Score: 2 * (precision * recall) / (precision + recall)
fn f1Score(y_true: array, y_pred: array) -> float {
    let p = precision(y_true, y_pred)
    let r = recall(y_true, y_pred)
    if (p + r == 0) { return 0.0 }
    return 2.0 * p * r / (p + r)
}

// F-beta Score
fn fbetaScore(y_true: array, y_pred: array, beta: float) -> float {
    let p = precision(y_true, y_pred)
    let r = recall(y_true, y_pred)
    let beta2 = beta * beta
    if ((beta2 * p) + r == 0) { return 0.0 }
    return (1 + beta2) * p * r / ((beta2 * p) + r)
}

// Specificity: TN / (TN + FP)
fn specificity(y_true: array, y_pred: array) -> float {
    let cm = confusionMatrix(y_true, y_pred)
    if (cm.tn + cm.fp == 0) { return 0.0 }
    return float(cm.tn) / (cm.tn + cm.fp)
}

// Balanced Accuracy: (sensitivity + specificity) / 2
fn balancedAccuracy(y_true: array, y_pred: array) -> float {
    let sens = recall(y_true, y_pred)
    let spec = specificity(y_true, y_pred)
    return (sens + spec) / 2.0
}

// Matthews Correlation Coefficient
fn matthewsCorrcoef(y_true: array, y_pred: array) -> float {
    let cm = confusionMatrix(y_true, y_pred)
    let tp = float(cm.tp)
    let tn = float(cm.tn)
    let fp = float(cm.fp)
    let fn = float(cm.fn)
    
    let numerator = tp * tn - fp * fn
    let denominator = sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
    
    if (denominator == 0) { return 0.0 }
    return numerator / denominator
}

// Cohen's Kappa
fn cohenKappa(y_true: array, y_pred: array) -> float {
    let n = len(y_true)
    let p0 = accuracy(y_true, y_pred)
    
    // Expected accuracy by chance
    let classes = unique(y_true)
    let pe = 0.0
    
    for (let c = 0; c < len(classes); c = c + 1) {
        let true_count = 0
        let pred_count = 0
        for (let i = 0; i < n; i = i + 1) {
            if (y_true[i] == classes[c]) { true_count = true_count + 1 }
            if (y_pred[i] == classes[c]) { pred_count = pred_count + 1 }
        }
        pe = pe + float(true_count * pred_count) / (n * n)
    }
    
    if (1 - pe == 0) { return 0.0 }
    return (p0 - pe) / (1 - pe)
}

// Log Loss (Cross-Entropy Loss)
fn logLoss(y_true: array, y_proba: array) -> float {
    let n = len(y_true)
    let loss = 0.0
    
    for (let i = 0; i < n; i = i + 1) {
        let p = y_proba[i]
        // Clip to avoid log(0)
        if (p < 1e-15) { p = 1e-15 }
        if (p > 1 - 1e-15) { p = 1 - 1e-15 }
        
        if (y_true[i] == 1) {
            loss = loss - ln(p)
        } else {
            loss = loss - ln(1 - p)
        }
    }
    
    return loss / n
}

// Multi-class accuracy
fn multiclassAccuracy(y_true: array, y_pred: array) -> float {
    return accuracy(y_true, y_pred)
}

// Macro-averaged precision
fn macroPrecision(y_true: array, y_pred: array) -> float {
    let classes = unique(y_true)
    let total = 0.0
    
    for (let c = 0; c < len(classes); c = c + 1) {
        let tp = 0
        let fp = 0
        for (let i = 0; i < len(y_true); i = i + 1) {
            if (y_pred[i] == classes[c]) {
                if (y_true[i] == classes[c]) {
                    tp = tp + 1
                } else {
                    fp = fp + 1
                }
            }
        }
        if (tp + fp > 0) {
            total = total + float(tp) / (tp + fp)
        }
    }
    
    return total / len(classes)
}

// Macro-averaged recall
fn macroRecall(y_true: array, y_pred: array) -> float {
    let classes = unique(y_true)
    let total = 0.0
    
    for (let c = 0; c < len(classes); c = c + 1) {
        let tp = 0
        let fn = 0
        for (let i = 0; i < len(y_true); i = i + 1) {
            if (y_true[i] == classes[c]) {
                if (y_pred[i] == classes[c]) {
                    tp = tp + 1
                } else {
                    fn = fn + 1
                }
            }
        }
        if (tp + fn > 0) {
            total = total + float(tp) / (tp + fn)
        }
    }
    
    return total / len(classes)
}

// Macro-averaged F1
fn macroF1(y_true: array, y_pred: array) -> float {
    let p = macroPrecision(y_true, y_pred)
    let r = macroRecall(y_true, y_pred)
    if (p + r == 0) { return 0.0 }
    return 2.0 * p * r / (p + r)
}

// ===== Regression Metrics =====

// Mean Absolute Error
fn meanAbsoluteError(y_true: array, y_pred: array) -> float {
    let total = 0.0
    for (let i = 0; i < len(y_true); i = i + 1) {
        total = total + abs(y_true[i] - y_pred[i])
    }
    return total / len(y_true)
}

// Mean Squared Error
fn meanSquaredError(y_true: array, y_pred: array) -> float {
    let total = 0.0
    for (let i = 0; i < len(y_true); i = i + 1) {
        let diff = y_true[i] - y_pred[i]
        total = total + diff * diff
    }
    return total / len(y_true)
}

// Root Mean Squared Error
fn rootMeanSquaredError(y_true: array, y_pred: array) -> float {
    return sqrt(meanSquaredError(y_true, y_pred))
}

// R² Score (Coefficient of Determination)
fn r2Score(y_true: array, y_pred: array) -> float {
    // Calculate mean of y_true
    let mean = 0.0
    for (let i = 0; i < len(y_true); i = i + 1) {
        mean = mean + y_true[i]
    }
    mean = mean / len(y_true)
    
    // SS_res = sum((y_true - y_pred)^2)
    let ss_res = 0.0
    for (let i = 0; i < len(y_true); i = i + 1) {
        let diff = y_true[i] - y_pred[i]
        ss_res = ss_res + diff * diff
    }
    
    // SS_tot = sum((y_true - mean)^2)
    let ss_tot = 0.0
    for (let i = 0; i < len(y_true); i = i + 1) {
        let diff = y_true[i] - mean
        ss_tot = ss_tot + diff * diff
    }
    
    if (ss_tot == 0) { return 0.0 }
    return 1.0 - ss_res / ss_tot
}

// Adjusted R² Score
fn adjustedR2Score(y_true: array, y_pred: array, n_features: int) -> float {
    let n = len(y_true)
    let r2 = r2Score(y_true, y_pred)
    return 1.0 - (1.0 - r2) * (n - 1) / (n - n_features - 1)
}

// Mean Absolute Percentage Error
fn meanAbsolutePercentageError(y_true: array, y_pred: array) -> float {
    let total = 0.0
    let count = 0
    for (let i = 0; i < len(y_true); i = i + 1) {
        if (y_true[i] != 0) {
            total = total + abs((y_true[i] - y_pred[i]) / y_true[i])
            count = count + 1
        }
    }
    if (count == 0) { return 0.0 }
    return total / count * 100
}

// Median Absolute Error
fn medianAbsoluteError(y_true: array, y_pred: array) -> float {
    let errors = []
    for (let i = 0; i < len(y_true); i = i + 1) {
        errors = append(errors, abs(y_true[i] - y_pred[i]))
    }
    errors = sort(errors)
    
    let n = len(errors)
    if (n % 2 == 0) {
        return (errors[n/2 - 1] + errors[n/2]) / 2
    }
    return errors[n/2]
}

// Max Error
fn maxError(y_true: array, y_pred: array) -> float {
    let max_err = 0.0
    for (let i = 0; i < len(y_true); i = i + 1) {
        let err = abs(y_true[i] - y_pred[i])
        if (err > max_err) { max_err = err }
    }
    return max_err
}

// Explained Variance Score
fn explainedVarianceScore(y_true: array, y_pred: array) -> float {
    let n = len(y_true)
    
    // Residuals
    let residuals = []
    for (let i = 0; i < n; i = i + 1) {
        residuals = append(residuals, y_true[i] - y_pred[i])
    }
    
    // Variance of residuals
    let mean_res = 0.0
    for (let i = 0; i < n; i = i + 1) {
        mean_res = mean_res + residuals[i]
    }
    mean_res = mean_res / n
    
    let var_res = 0.0
    for (let i = 0; i < n; i = i + 1) {
        let diff = residuals[i] - mean_res
        var_res = var_res + diff * diff
    }
    var_res = var_res / n
    
    // Variance of y_true
    let mean_y = 0.0
    for (let i = 0; i < n; i = i + 1) {
        mean_y = mean_y + y_true[i]
    }
    mean_y = mean_y / n
    
    let var_y = 0.0
    for (let i = 0; i < n; i = i + 1) {
        let diff = y_true[i] - mean_y
        var_y = var_y + diff * diff
    }
    var_y = var_y / n
    
    if (var_y == 0) { return 0.0 }
    return 1.0 - var_res / var_y
}

// ===== Clustering Metrics =====

// Silhouette Score
fn silhouetteScore(X: array, labels: array) -> float {
    let n = len(X)
    let total = 0.0
    
    for (let i = 0; i < n; i = i + 1) {
        // a(i) = mean distance to points in same cluster
        let a = 0.0
        let a_count = 0
        
        for (let j = 0; j < n; j = j + 1) {
            if (i != j && labels[i] == labels[j]) {
                a = a + euclideanDistance(X[i], X[j])
                a_count = a_count + 1
            }
        }
        if (a_count > 0) { a = a / a_count }
        
        // b(i) = min mean distance to points in other clusters
        let b = 999999.0
        let clusters = unique(labels)
        
        for (let c = 0; c < len(clusters); c = c + 1) {
            if (clusters[c] == labels[i]) { continue }
            
            let cluster_dist = 0.0
            let cluster_count = 0
            for (let j = 0; j < n; j = j + 1) {
                if (labels[j] == clusters[c]) {
                    cluster_dist = cluster_dist + euclideanDistance(X[i], X[j])
                    cluster_count = cluster_count + 1
                }
            }
            if (cluster_count > 0) {
                cluster_dist = cluster_dist / cluster_count
                if (cluster_dist < b) { b = cluster_dist }
            }
        }
        
        // Silhouette coefficient for point i
        let s = 0.0
        let max_ab = a
        if (b > max_ab) { max_ab = b }
        if (max_ab > 0) {
            s = (b - a) / max_ab
        }
        total = total + s
    }
    
    return total / n
}

// Davies-Bouldin Index (lower is better)
fn daviesBouldinScore(X: array, labels: array) -> float {
    let clusters = unique(labels)
    let k = len(clusters)
    
    // Calculate cluster centroids and scatter
    let centroids = []
    let scatter = []
    
    for (let c = 0; c < k; c = c + 1) {
        let points = []
        for (let i = 0; i < len(X); i = i + 1) {
            if (labels[i] == clusters[c]) {
                points = append(points, X[i])
            }
        }
        
        // Centroid
        let centroid = zeros(len(X[0]))
        for (let i = 0; i < len(points); i = i + 1) {
            for (let j = 0; j < len(centroid); j = j + 1) {
                centroid[j] = centroid[j] + points[i][j]
            }
        }
        for (let j = 0; j < len(centroid); j = j + 1) {
            centroid[j] = centroid[j] / len(points)
        }
        centroids = append(centroids, centroid)
        
        // Scatter (mean distance to centroid)
        let s = 0.0
        for (let i = 0; i < len(points); i = i + 1) {
            s = s + euclideanDistance(points[i], centroid)
        }
        s = s / len(points)
        scatter = append(scatter, s)
    }
    
    // Calculate DB index
    let db = 0.0
    for (let i = 0; i < k; i = i + 1) {
        let max_ratio = 0.0
        for (let j = 0; j < k; j = j + 1) {
            if (i != j) {
                let d_ij = euclideanDistance(centroids[i], centroids[j])
                if (d_ij > 0) {
                    let ratio = (scatter[i] + scatter[j]) / d_ij
                    if (ratio > max_ratio) { max_ratio = ratio }
                }
            }
        }
        db = db + max_ratio
    }
    
    return db / k
}

// Calinski-Harabasz Index (higher is better)
fn calinskiHarabaszScore(X: array, labels: array) -> float {
    let n = len(X)
    let clusters = unique(labels)
    let k = len(clusters)
    
    if (k <= 1) { return 0.0 }
    
    // Overall centroid
    let overall_centroid = zeros(len(X[0]))
    for (let i = 0; i < n; i = i + 1) {
        for (let j = 0; j < len(X[0]); j = j + 1) {
            overall_centroid[j] = overall_centroid[j] + X[i][j]
        }
    }
    for (let j = 0; j < len(overall_centroid); j = j + 1) {
        overall_centroid[j] = overall_centroid[j] / n
    }
    
    // Between-cluster dispersion
    let bgss = 0.0
    // Within-cluster dispersion
    let wgss = 0.0
    
    for (let c = 0; c < k; c = c + 1) {
        let points = []
        for (let i = 0; i < n; i = i + 1) {
            if (labels[i] == clusters[c]) {
                points = append(points, X[i])
            }
        }
        
        // Cluster centroid
        let centroid = zeros(len(X[0]))
        for (let i = 0; i < len(points); i = i + 1) {
            for (let j = 0; j < len(centroid); j = j + 1) {
                centroid[j] = centroid[j] + points[i][j]
            }
        }
        for (let j = 0; j < len(centroid); j = j + 1) {
            centroid[j] = centroid[j] / len(points)
        }
        
        // BGSS contribution
        let dist_to_overall = euclideanDistance(centroid, overall_centroid)
        bgss = bgss + len(points) * dist_to_overall * dist_to_overall
        
        // WGSS contribution
        for (let i = 0; i < len(points); i = i + 1) {
            let dist = euclideanDistance(points[i], centroid)
            wgss = wgss + dist * dist
        }
    }
    
    if (wgss == 0) { return 0.0 }
    return (bgss / (k - 1)) / (wgss / (n - k))
}

// ===== Cross-Validation =====

fn crossValScore(model: map, X: array, y: array, cv: int, metric: string, fitFn: fn, predictFn: fn) -> array {
    let n = len(X)
    let fold_size = int(n / cv)
    let scores = []
    
    for (let fold = 0; fold < cv; fold = fold + 1) {
        let test_start = fold * fold_size
        let test_end = test_start + fold_size
        if (fold == cv - 1) { test_end = n }
        
        // Split data
        let X_train = []
        let y_train = []
        let X_test = []
        let y_test = []
        
        for (let i = 0; i < n; i = i + 1) {
            if (i >= test_start && i < test_end) {
                X_test = append(X_test, X[i])
                y_test = append(y_test, y[i])
            } else {
                X_train = append(X_train, X[i])
                y_train = append(y_train, y[i])
            }
        }
        
        // Fit and predict
        model = fitFn(model, X_train, y_train)
        let y_pred = predictFn(model, X_test)
        
        // Calculate score
        let score = 0.0
        if (metric == "accuracy") {
            score = accuracy(y_test, y_pred)
        } else if (metric == "f1") {
            score = f1Score(y_test, y_pred)
        } else if (metric == "mse") {
            score = -meanSquaredError(y_test, y_pred)  // Negative because we maximize
        } else if (metric == "r2") {
            score = r2Score(y_test, y_pred)
        }
        
        scores = append(scores, score)
    }
    
    return scores
}

// ===== Utility Functions =====

fn unique(arr: array) -> array {
    let result = []
    for (let i = 0; i < len(arr); i = i + 1) {
        let found = false
        for (let j = 0; j < len(result); j = j + 1) {
            if (arr[i] == result[j]) {
                found = true
                break
            }
        }
        if (!found) {
            result = append(result, arr[i])
        }
    }
    return result
}

fn sort(arr: array) -> array {
    let result = []
    for (let i = 0; i < len(arr); i = i + 1) {
        result = append(result, arr[i])
    }
    for (let i = 0; i < len(result) - 1; i = i + 1) {
        for (let j = 0; j < len(result) - i - 1; j = j + 1) {
            if (result[j] > result[j + 1]) {
                let temp = result[j]
                result[j] = result[j + 1]
                result[j + 1] = temp
            }
        }
    }
    return result
}

fn euclideanDistance(a: array, b: array) -> float {
    let sum = 0.0
    for (let i = 0; i < len(a); i = i + 1) {
        let diff = a[i] - b[i]
        sum = sum + diff * diff
    }
    return sqrt(sum)
}

fn zeros(n: int) -> array {
    let result = []
    for (let i = 0; i < n; i = i + 1) {
        result = append(result, 0.0)
    }
    return result
}

// Classification report (returns metrics for all classes)
fn classificationReport(y_true: array, y_pred: array) -> map {
    let classes = unique(y_true)
    let report = {
        "accuracy": accuracy(y_true, y_pred),
        "macro_precision": macroPrecision(y_true, y_pred),
        "macro_recall": macroRecall(y_true, y_pred),
        "macro_f1": macroF1(y_true, y_pred)
    }
    return report
}
